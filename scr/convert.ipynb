{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Convertir el modelo a formato ONNX\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m640\u001b[39m, \u001b[39m640\u001b[39m)  \u001b[39m# Puedes ajustar las dimensiones seg√∫n tus datos de entrada\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, dummy_input, \u001b[39m'\u001b[39;49m\u001b[39m/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m, opset_version\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/site-packages/torch/onnx/utils.py:1573\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, (torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule)):\n\u001b[1;32m   1569\u001b[0m     module_typenames_to_export_as_functions \u001b[39m=\u001b[39m _setup_trace_module_map(\n\u001b[1;32m   1570\u001b[0m         model, export_modules_as_functions\n\u001b[1;32m   1571\u001b[0m     )\n\u001b[0;32m-> 1573\u001b[0m \u001b[39mwith\u001b[39;00m exporter_context(model, training, verbose):\n\u001b[1;32m   1574\u001b[0m     val_keep_init_as_ip \u001b[39m=\u001b[39m _decide_keep_init_as_input(\n\u001b[1;32m   1575\u001b[0m         keep_initializers_as_inputs,\n\u001b[1;32m   1576\u001b[0m         operator_export_type,\n\u001b[1;32m   1577\u001b[0m         opset_version,\n\u001b[1;32m   1578\u001b[0m     )\n\u001b[1;32m   1579\u001b[0m     val_add_node_names \u001b[39m=\u001b[39m _decide_add_node_names(\n\u001b[1;32m   1580\u001b[0m         add_node_names, operator_export_type\n\u001b[1;32m   1581\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/site-packages/torch/onnx/utils.py:179\u001b[0m, in \u001b[0;36mexporter_context\u001b[0;34m(model, mode, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m    177\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexporter_context\u001b[39m(model, mode: _C_onnx\u001b[39m.\u001b[39mTrainingMode, verbose: \u001b[39mbool\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mwith\u001b[39;00m select_model_mode_for_export(\n\u001b[1;32m    180\u001b[0m         model, mode\n\u001b[1;32m    181\u001b[0m     ) \u001b[39mas\u001b[39;00m mode_ctx, disable_apex_o2_state_dict_hook(\n\u001b[1;32m    182\u001b[0m         model\n\u001b[1;32m    183\u001b[0m     ) \u001b[39mas\u001b[39;00m apex_ctx, setup_onnx_logging(\n\u001b[1;32m    184\u001b[0m         verbose\n\u001b[1;32m    185\u001b[0m     ) \u001b[39mas\u001b[39;00m log_ctx, diagnostics\u001b[39m.\u001b[39mcreate_export_diagnostic_context() \u001b[39mas\u001b[39;00m diagnostic_ctx:\n\u001b[1;32m    186\u001b[0m         \u001b[39myield\u001b[39;00m (mode_ctx, apex_ctx, log_ctx, diagnostic_ctx)\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/trainmodel/lib/python3.10/site-packages/torch/onnx/utils.py:140\u001b[0m, in \u001b[0;36mdisable_apex_o2_state_dict_hook\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction):\n\u001b[1;32m    139\u001b[0m     model_hooks \u001b[39m=\u001b[39m {}  \u001b[39m# type: ignore[var-annotated]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39;49mmodules():\n\u001b[1;32m    141\u001b[0m         \u001b[39mfor\u001b[39;00m key, hook \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_state_dict_hooks\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    142\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(hook)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO2StateDictHook\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'modules'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "\n",
    "# Cargar el modelo YOLO preentrenado\n",
    "model = torch.load('/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt')\n",
    "\n",
    "# Convertir el modelo a formato ONNX\n",
    "dummy_input = torch.randn(1, 3, 640, 640)  # Puedes ajustar las dimensiones seg√∫n tus datos de entrada\n",
    "torch.onnx.export(model, dummy_input, '/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx', opset_version=11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Ejecutar el modelo en modo de evaluaci√≥n\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Crear una entrada de ejemplo\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m example_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m640\u001b[39m, \u001b[39m640\u001b[39m)  \u001b[39m# Ajusta las dimensiones seg√∫n tus datos de entrada\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import ultralytics\n",
    "\n",
    "# Cargar el modelo YOLO preentrenado\n",
    "model = torch.load('/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt')\n",
    "\n",
    "# Ejecutar el modelo en modo de evaluaci√≥n\n",
    "model.eval()\n",
    "\n",
    "# Crear una entrada de ejemplo\n",
    "example_input = torch.randn(1, 3, 640, 640)  # Ajusta las dimensiones seg√∫n tus datos de entrada\n",
    "\n",
    "# Exportar el modelo a TorchScript\n",
    "traced_script_model = torch.jit.trace(model, example_input)\n",
    "traced_script_model.save('/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/bestJIT.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Cargar el modelo TorchScript\n",
    "model = torch.jit.load('yolov5s_script_model.pt')\n",
    "\n",
    "# Preparar datos de entrada\n",
    "input_data = torch.randn(1, 3, 640, 640)  # Ajusta las dimensiones seg√∫n tus datos de entrada\n",
    "\n",
    "# Realizar inferencia\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb Celda 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Cargar el modelo previamente entrenado\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Cargar una imagen de entrada (reemplaza 'input_image.jpg' con tu ruta)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Vision/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1015\u001b[0m                      map_location,\n\u001b[1;32m   1016\u001b[0m                      pickle_module,\n\u001b[1;32m   1017\u001b[0m                      overall_storage\u001b[39m=\u001b[39;49moverall_storage,\n\u001b[1;32m   1018\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmmap can only be used with files saved with \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Vision/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1424\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.load.metadata\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mserialization_id\u001b[39m\u001b[39m\"\u001b[39m: zip_file\u001b[39m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/Vision/lib/python3.11/site-packages/torch/serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1415\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Cargar el modelo previamente entrenado\n",
    "model = torch.load(\"/home/hackbrian/Documentos/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt\")\n",
    "model.eval()\n",
    "\n",
    "# Cargar una imagen de entrada (reemplaza 'input_image.jpg' con tu ruta)\n",
    "input_image_path = '/home/hackbrian/Descargas/data/dataV1/images/ground/057.jpg'\n",
    "input_image = cv2.imread(input_image_path)\n",
    "input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)  # Convertir a formato RGB\n",
    "\n",
    "# Preprocesar la imagen para que coincida con el formato de entrada del modelo\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "input_image = transform(input_image).unsqueeze(0)  # A√±adir dimensi√≥n del lote\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "\n",
    "# Mostrar los resultados\n",
    "for prediction in predictions[0]['boxes']:\n",
    "    print(\"Box:\", prediction.numpy())\n",
    "\n",
    "# Puedes continuar con el c√≥digo para mostrar las predicciones en la imagen utilizando OpenCV, matplotlib, u otra biblioteca de visualizaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.217 üöÄ Python-3.10.9 torch-2.1.1+cu121 CPU (Intel Core(TM) i7-7700HQ 2.80GHz)\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 37, 8400), (1, 32, 160, 160)) (6.5 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in /home/hackbrian/anaconda3/envs/trainmodel/lib/python3.10/site-packages (from onnx>=1.12.0) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/hackbrian/anaconda3/envs/trainmodel/lib/python3.10/site-packages (from onnx>=1.12.0) (4.25.1)\n",
      "Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.15.0\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 15.3s, installed 1 package: ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 16.3s, saved as '/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx' (12.6 MB)\n",
      "\n",
      "Export complete (18.3s)\n",
      "Results saved to \u001b[1m/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights\u001b[0m\n",
      "Predict:         yolo predict task=segment model=/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx imgsz=640 data=dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.onnx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO('yolov8n.pt')  # load an official model\n",
    "model = YOLO('/home/hackbrian/gitProyects/Seeker_ComputerVision/YoloTrain/runs/segment/train/weights/best.pt')  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Required inputs (['images']) are missing from input feed (['cubo']).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m input_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(input_data, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Realiza la inferencia en el fotograma preprocesado\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, {\u001b[39m'\u001b[39;49m\u001b[39mcubo\u001b[39;49m\u001b[39m'\u001b[39;49m: input_data})  \u001b[39m# Reemplaza 'input_name' con el nombre correcto\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Postprocesamiento del resultado de la inferencia\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Aqu√≠, se utiliza un postprocesamiento b√°sico: umbralizaci√≥n para la segmentaci√≥n\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hackbrian/gitProyects/Seeker_ComputerVision/scr/convert.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:216\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, output_names, input_feed, run_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    Compute the predictions.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m        sess.run([output_name], {input_name: x})\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(\u001b[39mlist\u001b[39;49m(input_feed\u001b[39m.\u001b[39;49mkeys()))\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_names:\n\u001b[1;32m    218\u001b[0m         output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n",
      "File \u001b[0;32m~/anaconda3/envs/vision/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:198\u001b[0m, in \u001b[0;36mSession._validate_input\u001b[0;34m(self, feed_input_names)\u001b[0m\n\u001b[1;32m    196\u001b[0m         missing_input_names\u001b[39m.\u001b[39mappend(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m missing_input_names:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequired inputs (\u001b[39m\u001b[39m{\u001b[39;00mmissing_input_names\u001b[39m}\u001b[39;00m\u001b[39m) are missing from input feed (\u001b[39m\u001b[39m{\u001b[39;00mfeed_input_names\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Required inputs (['images']) are missing from input feed (['cubo'])."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
